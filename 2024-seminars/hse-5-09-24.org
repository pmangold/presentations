In this paper, we analyze the sample and communication complexity of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the effects of local training with agent heterogeneity. We show that the communication complexity of FedLSA scales polynomially with the inverse of the desired accuracy ϵ. To overcome this, we propose SCAFFLSA a new variant of FedLSA that uses control variates to correct for client drift, and establish its sample and communication complexities. We show that for statistically heterogeneous agents, its communication complexity scales logarithmically with the desired accuracy, similar to Scaffnew. An important finding is that, compared to the existing results for Scaffnew, the sample complexity scales with the inverse of the number of agents, a property referred to as linear speed-up. Achieving this linear speed-up requires completely new theoretical arguments. We apply the proposed method to federated temporal difference learning with linear function approximation and analyze the corresponding complexity improvements.



Federated linear stochastic approximation (FedLSA) enables decentralized agents to collaborate on optimization while keeping data local. In this talk, I will discuss the sample and communication complexity of FedLSA, highlighting how its communication complexity scales polynomially with the inverse of the desired accuracy ϵ. To address this, I'll introduce SCAFFLSA, a new variant that uses control variates to reduce client drift, achieving logarithmic scaling in communication complexity for heterogeneous agents. Notably, SCAFFLSA also provides a linear speed-up in sample complexity, scaling inversely with the number of agents.

Title: Taming Heterogeneity in Federated Linear Stochastic Approximation

Abstract:
In federated learning, multiple agents collaboratively train a machine learning model without exchanging local data. To achieve this, each agent locally updates a global model, and the updated models are periodically aggregated. In this talk, I will focus on federated linear stochastic approximation (FedLSA), with a strong focus on agents heterogeneity. I will derive upper bounds on the sample and communication complexity of FedLSA, and present a new method to reduce communication cost using control variates. Particular attention will be put on the "linear speed-up" phenomenon, showing that the sample complexity scales with the inverse of the number of agents in both methods.
