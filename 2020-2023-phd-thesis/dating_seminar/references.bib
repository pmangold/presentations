@article{bassily2014Differentially,
  title = {Differentially {{Private Empirical Risk Minimization}}: {{Efficient Algorithms}} and {{Tight Error Bounds}}},
  shorttitle = {Differentially {{Private Empirical Risk Minimization}}},
  author = {Bassily, Raef and Smith, Adam and Thakurta, Abhradeep},
  year = {2014},
  month = oct,
  journal = {arXiv:1405.7085 [cs, stat]},
  eprint = {1405.7085},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1405.7085},
  urldate = {2021-09-07},
  abstract = {In this paper, we initiate a systematic investigation of differentially private algorithms for convex empirical risk minimization. Various instantiations of this problem have been studied before. We provide new algorithms and matching lower bounds for private ERM assuming only that each data point's contribution to the loss function is Lipschitz bounded and that the domain of optimization is bounded. We provide a separate set of algorithms and matching lower bounds for the setting in which the loss functions are known to also be strongly convex.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/pmangold/research/references/pdf/bassily_et_al_2014_differentially_private_empirical_risk_minimization3.pdf}
}

@article{beck2009Fast,
  title = {A {{Fast Iterative Shrinkage-Thresholding Algorithm}} for {{Linear Inverse Problems}}},
  author = {Beck, Amir and Teboulle, Marc},
  year = {2009},
  pages = {20},
  abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.},
  langid = {english},
  file = {/home/pmangold/research/references/pdf/beck_teboulle_a_fast_iterative_shrinkage-thresholding_algorithm_for_linear_inverse_problems.pdf}
}

@article{candes2008Enhancing,
  title = {Enhancing {{Sparsity}} by {{Reweighted L1 Minimization}}},
  author = {Cand{\`e}s, Emmanuel J. and Wakin, Michael B. and Boyd, Stephen P.},
  year = {2008},
  month = dec,
  journal = {Journal of Fourier Analysis and Applications},
  volume = {14},
  number = {5},
  pages = {877--905},
  issn = {1531-5851},
  doi = {10.1007/s00041-008-9045-x},
  url = {https://doi.org/10.1007/s00041-008-9045-x},
  urldate = {2022-02-16},
  abstract = {It is now well understood that (1) it is possible to reconstruct sparse signals exactly from what appear to be highly incomplete sets of linear measurements and (2)~that this can be done by constrained {$\mathscr{l}$}1 minimization. In this paper, we study a novel method for sparse signal recovery that in many situations outperforms {$\mathscr{l}$}1 minimization in the sense that substantially fewer measurements are needed for exact recovery. The algorithm consists of solving a sequence of weighted {$\mathscr{l}$}1-minimization problems where the weights used for the next iteration are computed from the value of the current solution. We present a series of experiments demonstrating the remarkable performance and broad applicability of this algorithm in the areas of sparse signal recovery, statistical estimation, error correction and image processing. Interestingly, superior gains are also achieved when our method is applied to recover signals with assumed near-sparsity in overcomplete representations\textemdash not by reweighting the {$\mathscr{l}$}1 norm of the coefficient sequence as is common, but by reweighting the {$\mathscr{l}$}1 norm of the transformed object. An immediate consequence is the possibility of highly efficient data acquisition protocols by improving on a technique known as Compressive Sensing.},
  langid = {english},
  file = {/home/pmangold/research/references/pdf/candès_et_al_2008_enhancing_sparsity_by_reweighted_ℓ1_minimization.pdf}
}

@article{chaudhuri2011Differentially,
  title = {Differentially {{Private Empirical Risk Minimization}}},
  author = {Chaudhuri, Kamalika and Monteleoni, Claire and Sarwate, Anand D.},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {29},
  pages = {1069--1109},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v12/chaudhuri11a.html},
  urldate = {2021-06-29},
  file = {/home/pmangold/research/references/pdf/chaudhuri_et_al_2011_differentially_private_empirical_risk_minimization2.pdf;/home/pmangold/zotero/storage/67E5FYTK/chaudhuri11a.html}
}

@article{clarkson2010Coresets,
  title = {Coresets, Sparse Greedy Approximation, and the {{Frank-Wolfe}} Algorithm},
  author = {Clarkson, Kenneth L.},
  year = {2010},
  month = aug,
  journal = {ACM Transactions on Algorithms},
  volume = {6},
  number = {4},
  pages = {1--30},
  issn = {1549-6325, 1549-6333},
  doi = {10.1145/1824777.1824783},
  url = {https://dl.acm.org/doi/10.1145/1824777.1824783},
  urldate = {2022-02-07},
  abstract = {The problem of maximizing a concave function f (x) in a simplex S can be solved approximately by a simple greedy algorithm. For given k, the algorithm can find a point x(k) on a k-dimensional face of S, such that f (x(k)) {$\geq$} f (x{${_\ast}$}) - O(1/k). Here f (x{${_\ast}$}) is the maximum value of f in S. This algorithm and analysis were known before, and related to problems of statistics and machine learning, such as boosting, regression, and density mixture estimation. In other work, coming from computational geometry, the existence of -coresets was shown for the minimum enclosing ball problem, by means of a simple greedy algorithm. Similar greedy algorithms, that are special cases of the Frank-Wolfe algorithm, were described for other enclosure problems. Here these results are tied together, stronger convergence results are reviewed, and several coreset bounds are generalized or strengthened.},
  langid = {english},
  file = {/home/pmangold/research/references/pdf/clarkson_2010_coresets,_sparse_greedy_approximation,_and_the_frank-wolfe_algorithm.pdf;/home/pmangold/zotero/storage/UVD4PCXM/Clarkson - 2010 - Coresets, sparse greedy approximation, and the Fra.pdf}
}

@inproceedings{dwork2006Differential,
  title = {Differential {{Privacy}}},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Dwork, Cynthia},
  editor = {Bugliesi, Michele and Preneel, Bart and Sassone, Vladimiro and Wegener, Ingo},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--12},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11787006_1},
  url = {https://link.springer.com/chapter/10.1007%2F11787006_1},
  abstract = {In 1977 Dalenius articulated a desideratum for statistical databases: nothing about an individual should be learnable from the database that cannot be learned without access to the database. We give a general impossibility result showing that a formalization of Dalenius' goal along the lines of semantic security cannot be achieved. Contrary to intuition, a variant of the result threatens the privacy even of someone not in the database. This state of affairs suggests a new measure, differential privacy, which, intuitively, captures the increased risk to one's privacy incurred by participating in a database. The techniques developed in a sequence of papers [8, 13, 3], culminating in those described in [12], can achieve any desired level of privacy under this measure. In many cases, extremely accurate information about the database can be provided while simultaneously ensuring very high levels of privacy.},
  isbn = {978-3-540-35908-1},
  langid = {english},
  keywords = {Auxiliary Information,Differential Privacy,Impossibility Result,Statistical Database,Turing Machine},
  file = {/home/pmangold/research/references/pdf/dwork_2006_differential_privacy.pdf;/home/pmangold/research/references/pdf/dwork_2006_differential_privacy2.pdf}
}

@inproceedings{fang2020Greed,
  title = {Greed {{Meets Sparsity}}: {{Understanding}} and {{Improving Greedy Coordinate Descent}} for {{Sparse Optimization}}},
  shorttitle = {Greed {{Meets Sparsity}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Fang, Huang and Fan, Zhenan and Sun, Yifan and Friedlander, Michael},
  year = {2020},
  month = jun,
  pages = {434--444},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v108/fang20a.html},
  urldate = {2021-11-04},
  abstract = {We consider greedy coordinate descent (GCD) for composite problems with sparsity inducing regularizers, including 1-norm regularization and non-negative constraints. Empirical evidence strongly suggests that GCD, when initialized with the zero vector, has an implicit screening ability that usually selects at each iteration coordinates that at are nonzero at the solution. Thus, for problems with sparse solutions, GCD can converge significantly faster than randomized coordinate descent. We present an improved convergence analysis of GCD for sparse optimization, and a formal analysis of its screening properties. We also propose and analyze an improved selection rule with stronger ability to produce sparse iterates. Numerical experiments on both synthetic and real-world data support our analysis and the effectiveness of the proposed selection rule.},
  langid = {english},
  file = {/home/pmangold/research/references/pdf/fang_et_al_2020_greed_meets_sparsity.pdf;/home/pmangold/zotero/storage/AM6NYZPL/fang20a-supp.pdf;/home/pmangold/zotero/storage/L3Z9BYX6/Fang et al. - 2020 - Greed Meets Sparsity Understanding and Improving .pdf}
}

@article{holland1977Robust,
  title = {Robust Regression Using Iteratively Reweighted Least-Squares},
  author = {Holland, Paul W. and Welsch, Roy E.},
  year = {1977},
  month = jan,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {6},
  number = {9},
  pages = {813--827},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0926},
  doi = {10.1080/03610927708827533},
  url = {https://doi.org/10.1080/03610927708827533},
  urldate = {2022-01-18},
  abstract = {The rapid development of the theory of robust estimation (Huber, 1973) has created a need for computational procedures to produce robust estimates. We will review a number of different computational approaches for robust linear regression but focus on one\textemdash iteratively reweighted least-squares (IRLS). The weight functions that we discuss are a part of a semi-portable subroutine library called ROSEPACK (RObust Statistical Estimation PACKage) that has been developed by the authors and Virginia Klema at the Computer Research Center of the National Bureau of Economic Research, Inc. in Cambridge, Mass. with the support of the National Science Foundation. This library (Klema, 1976) makes it relatively simple to implement an IRLS regression package.},
  keywords = {one-step estimates,robust weight functions,ROSEPACK,SLASH distribution,small sample variances,tuning constants},
  annotation = {\_eprint: https://doi.org/10.1080/03610927708827533},
  file = {/home/pmangold/zotero/storage/27WULKL2/03610927708827533.html}
}

@inproceedings{karimireddy2019Efficient,
  title = {Efficient {{Greedy Coordinate Descent}} for {{Composite Problems}}},
  booktitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Karimireddy, Sai Praneeth and Koloskova, Anastasia and Stich, Sebastian U. and Jaggi, Martin},
  year = {2019},
  month = apr,
  pages = {2887--2896},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v89/karimireddy19a.html},
  urldate = {2021-06-29},
  abstract = {Coordinate descent with random coordinate selection is the current state of the art for many large scale optimization problems. However, greedy selection of the steepest coordinate on smooth proble...},
  langid = {english},
  file = {/home/pmangold/research/references/pdf/karimireddy_et_al_2019_efficient_greedy_coordinate_descent_for_composite_problems.pdf;/home/pmangold/zotero/storage/7P3TC8VC/karimireddy19a.html}
}

@article{mangold2021Differentially,
  title = {Differentially {{Private Coordinate Descent}} for {{Composite Empirical Risk Minimization}}},
  author = {Mangold, Paul and Bellet, Aur{\'e}lien and Salmon, Joseph and Tommasi, Marc},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.11688 [cs, stat]},
  eprint = {2110.11688},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2110.11688},
  urldate = {2021-11-05},
  abstract = {Machine learning models can leak information about the data used to train them. Differentially Private (DP) variants of optimization algorithms like Stochastic Gradient Descent (DP-SGD) have been designed to mitigate this, inducing a trade-off between privacy and utility. In this paper, we propose a new method for composite Differentially Private Empirical Risk Minimization (DP-ERM): Differentially Private proximal Coordinate Descent (DP-CD). We analyze its utility through a novel theoretical analysis of inexact coordinate descent, and highlight some regimes where DP-CD outperforms DP-SGD, thanks to the possibility of using larger step sizes. We also prove new lower bounds for composite DP-ERM under coordinate-wise regularity assumptions, that are, in some settings, nearly matched by our algorithm. In practical implementations, the coordinate-wise nature of DP-CD updates demands special care in choosing the clipping thresholds used to bound individual contributions to the gradients. A natural parameterization of these thresholds emerges from our theory, limiting the addition of unnecessarily large noise without requiring coordinate-wise hyperparameter tuning or extra computational cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/pmangold/research/references/pdf/mangold_et_al_2021_differentially_private_coordinate_descent_for_composite_empirical_risk.pdf;/home/pmangold/zotero/storage/N6XB8DFW/2110.html}
}

@inproceedings{massias2018Celer,
  title = {Celer: A {{Fast Solver}} for the {{Lasso}} with {{Dual Extrapolation}}},
  shorttitle = {Celer},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Massias, Mathurin and Gramfort, Alexandre and Salmon, Joseph},
  year = {2018},
  month = jul,
  pages = {3315--3324},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/massias18a.html},
  urldate = {2022-02-15},
  abstract = {Convex sparsity-inducing regularizations are ubiquitous in high-dimensional machine learning, but solving the resulting optimization problems can be slow. To accelerate solvers, state-of-the-art approaches consist in reducing the size of the optimization problem at hand. In the context of regression, this can be achieved either by discarding irrelevant features (screening techniques) or by prioritizing features likely to be included in the support of the solution (working set techniques). Duality comes into play at several steps in these techniques. Here, we propose an extrapolation technique starting from a sequence of iterates in the dual that leads to the construction of improved dual points. This enables a tighter control of optimality as used in stopping criterion, as well as better screening performance of Gap Safe rules. Finally, we propose a working set strategy based on an aggressive use of Gap Safe screening rules. Thanks to our new dual point construction, we show significant computational speedups on multiple real-world problems.},
  langid = {english},
  file = {/home/pmangold/research/references/pdf/massias_et_al_2018_celer.pdf;/home/pmangold/zotero/storage/U2MLVNRG/Massias et al. - 2018 - Celer a Fast Solver for the Lasso with Dual Extra.pdf}
}

@article{mironov2019Renyi,
  title = {R\'enyi {{Differential Privacy}} of the {{Sampled Gaussian Mechanism}}},
  author = {Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.10530 [cs, stat]},
  eprint = {1908.10530},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1908.10530},
  urldate = {2021-11-21},
  abstract = {The Sampled Gaussian Mechanism (SGM)---a composition of subsampling and the additive Gaussian noise---has been successfully used in a number of machine learning applications. The mechanism's unexpected power is derived from privacy amplification by sampling where the privacy cost of a single evaluation diminishes quadratically, rather than linearly, with the sampling rate. Characterizing the precise privacy properties of SGM motivated development of several relaxations of the notion of differential privacy. This work unifies and fills in gaps in published results on SGM. We describe a numerically stable procedure for precise computation of SGM's R\textbackslash 'enyi Differential Privacy and prove a nearly tight (within a small constant factor) closed-form bound.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/pmangold/research/references/pdf/mironov_et_al_2019_r-'enyi_differential_privacy_of_the_sampled_gaussian_mechanism.pdf;/home/pmangold/zotero/storage/4GKYFQE8/1908.html}
}

@article{mishchenko2021Proximal,
  title = {Proximal and {{Federated Random Reshuffling}}},
  author = {Mishchenko, Konstantin and Khaled, Ahmed and Richt{\'a}rik, Peter},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.06704 [cs, math]},
  eprint = {2102.06704},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  url = {http://arxiv.org/abs/2102.06704},
  urldate = {2022-01-13},
  abstract = {Random Reshuffling (RR), also known as Stochastic Gradient Descent (SGD) without replacement, is a popular and theoretically grounded method for finite-sum minimization. We propose two new algorithms: Proximal and Federated Random Reshuffing (ProxRR and FedRR). The first algorithm, ProxRR, solves composite convex finite-sum minimization problems in which the objective is the sum of a (potentially non-smooth) convex regularizer and an average of \$n\$ smooth objectives. We obtain the second algorithm, FedRR, as a special case of ProxRR applied to a reformulation of distributed problems with either homogeneous or heterogeneous data. We study the algorithms' convergence properties with constant and decreasing stepsizes, and show that they have considerable advantages over Proximal and Local SGD. In particular, our methods have superior complexities and ProxRR evaluates the proximal operator once per epoch only. When the proximal operator is expensive to compute, this small difference makes ProxRR up to \$n\$ times faster than algorithms that evaluate the proximal operator in every iteration. We give examples of practical optimization tasks where the proximal operator is difficult to compute and ProxRR has a clear advantage. Finally, we corroborate our results with experiments on real data sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/home/pmangold/research/references/pdf/mishchenko_et_al_2021_proximal_and_federated_random_reshuffling.pdf;/home/pmangold/zotero/storage/P3BT662K/2102.html}
}

@inproceedings{nutini2015Coordinate,
  title = {Coordinate {{Descent Converges Faster}} with the {{Gauss-Southwell Rule Than Random Selection}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Nutini, Julie and Schmidt, Mark and Laradji, Issam and Friedlander, Michael and Koepke, Hoyt},
  year = {2015},
  month = jun,
  pages = {1632--1641},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {http://proceedings.mlr.press/v37/nutini15.html},
  urldate = {2021-06-29},
  abstract = {There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of~ Nesterov [SIAM J. Optim., 22(2), 2012], who showed that...},
  langid = {english},
  file = {/home/pmangold/research/references/pdf/nutini_et_al_2015_coordinate_descent_converges_faster_with_the_gauss-southwell_rule_than_random2.pdf;/home/pmangold/zotero/storage/B57TIC4V/nutini15.html}
}

@article{pichapati2019AdaCliP,
  title = {{{AdaCliP}}: {{Adaptive Clipping}} for {{Private SGD}}},
  shorttitle = {{{AdaCliP}}},
  author = {Pichapati, Venkatadheeraj and Suresh, Ananda Theertha and Yu, Felix X. and Reddi, Sashank J. and Kumar, Sanjiv},
  year = {2019},
  month = oct,
  journal = {arXiv:1908.07643 [cs, stat]},
  eprint = {1908.07643},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1908.07643},
  urldate = {2020-04-07},
  abstract = {Privacy preserving machine learning algorithms are crucial for learning models over user data to protect sensitive information. Motivated by this, differentially private stochastic gradient descent (SGD) algorithms for training machine learning models have been proposed. At each step, these algorithms modify the gradients and add noise proportional to the sensitivity of the modified gradients. Under this framework, we propose AdaCliP, a theoretically motivated differentially private SGD algorithm that provably adds less noise compared to the previous methods, by using coordinate-wise adaptive clipping of the gradient. We empirically demonstrate that AdaCliP reduces the amount of added noise and produces models with better accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/pmangold/research/references/pdf/pichapati_et_al_2019_adaclip.pdf;/home/pmangold/zotero/storage/SK2JQETW/1908.html}
}

@article{richtarik2014Iteration,
  title = {Iteration Complexity of Randomized Block-Coordinate Descent Methods for Minimizing a Composite Function},
  author = {Richt{\'a}rik, Peter and Tak{\'a}{\v c}, Martin},
  year = {2014},
  month = apr,
  journal = {Mathematical Programming},
  volume = {144},
  number = {1-2},
  pages = {1--38},
  issn = {0025-5610, 1436-4646},
  doi = {10.1007/s10107-012-0614-z},
  url = {http://link.springer.com/10.1007/s10107-012-0614-z},
  urldate = {2019-10-22},
  abstract = {In this paper we develop a randomized block-coordinate descent method for minimizing the sum of a smooth and a simple nonsmooth block-separable convex function and prove that it obtains an {$\epsilon$}-accurate solution with probability at least 1 - {$\rho$} in at most O((n/{$\epsilon$}) log(1/{$\rho$})) iterations, where n is the number of blocks. This extends recent results of Nesterov [Efficiency of coordinate descent methods on huge-scale optimization problems, SIAM J Optimization 22(2), pp. 341\textendash 362, 2012], which cover the smooth case, to composite minimization, while at the same time improving the complexity by the factor of 4 and removing {$\epsilon$} from the logarithmic term. More importantly, in contrast with the aforementioned work in which the author achieves the results by applying the method to a regularized version of the objective function with an unknown scaling factor, we show that this is not necessary, thus achieving first true iteration complexity bounds. For strongly convex functions the method converges linearly. In the smooth case we also allow for arbitrary probability vectors and non-Euclidean norms. Finally, we demonstrate numerically that the algorithm is able to solve huge-scale {$\mathscr{l}$}1-regularized least squares with a billion variables.},
  langid = {english},
  keywords = {favorite},
  file = {/home/pmangold/research/references/pdf/richtárik_takáč_2014_iteration_complexity_of_randomized_block-coordinate_descent_methods_for.pdf}
}

@inproceedings{wang2017Differentially,
  title = {Differentially Private Empirical Risk Minimization Revisited: {{Faster}} and More General},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wang, Di and Ye, Minwei and Xu, Jinhui},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/file/f337d999d9ad116a7b4f3d409fcc6480-Paper.pdf},
  file = {/home/pmangold/research/references/pdf/wang_et_al_2017_differentially_private_empirical_risk_minimization_revisited3.pdf}
}

@article{xu2020Removing,
  title = {Removing {{Disparate Impact}} of {{Differentially Private Stochastic Gradient Descent}} on {{Model Accuracy}}},
  author = {Xu, Depeng and Du, Wei and Wu, Xintao},
  year = {2020},
  month = sep,
  journal = {arXiv:2003.03699 [cs, stat]},
  eprint = {2003.03699},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2003.03699},
  urldate = {2021-02-11},
  abstract = {When we enforce differential privacy in machine learning, the utility-privacy trade-off is different w.r.t. each group. Gradient clipping and random noise addition disproportionately affect underrepresented and complex classes and subgroups, which results in inequality in utility loss. In this work, we analyze the inequality in utility loss by differential privacy and propose a modified differentially private stochastic gradient descent (DPSGD), called DPSGD-F, to remove the potential disparate impact of differential privacy on the protected group. DPSGD-F adjusts the contribution of samples in a group depending on the group clipping bias such that differential privacy has no disparate impact on group utility. Our experimental evaluation shows how group sample size and group clipping bias affect the impact of differential privacy in DPSGD, and how adaptive clipping for each group helps to mitigate the disparate impact caused by differential privacy in DPSGD-F.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/pmangold/research/references/pdf/xu_et_al_2020_removing_disparate_impact_of_differentially_private_stochastic_gradient_descent.pdf}
}

@inproceedings{geiping2020Inverting,
	title = {Inverting {Gradients} - {How} easy is it to break privacy in federated learning?},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/c4ede56bbd98819ae6112b20ac6bf145-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Geiping, Jonas and Bauermeister, Hartmut and Dröge, Hannah and Moeller, Michael},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {16937--16947},
	file = {geiping_et_al_2020_inverting_gradients_-_how_easy_is_it_to_break_privacy_in_federated_learning.pdf:/home/pmangold/research/references/pdf/geiping_et_al_2020_inverting_gradients_-_how_easy_is_it_to_break_privacy_in_federated_learning.pdf:application/pdf},
}
