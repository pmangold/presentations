@article{abadi2016Deep,
  title = {Deep {{Learning}} with {{Differential Privacy}}},
  author = {Abadi, Mart{\'i}n and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  year = {2016},
  pages = {308--318},
  doi = {10.1145/2976749.2978318},
  url = {http://arxiv.org/abs/1607.00133},
  urldate = {2019-12-04},
  abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
  archiveprefix = {arXiv},
  eprint = {1607.00133},
  eprinttype = {arxiv},
  file = {/home/pmangold/Documents/references/pdf/abadi_et_al_2016_deep_learning_with_differential_privacy.pdf},
  journal = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security - CCS'16},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning,to read},
  language = {en}
}

@inproceedings{bassily2014Private,
  title = {Private {{Empirical Risk Minimization}}: {{Efficient Algorithms}} and {{Tight Error Bounds}}},
  shorttitle = {Private {{Empirical Risk Minimization}}},
  booktitle = {2014 {{IEEE}} 55th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Bassily, Raef and Smith, Adam and Thakurta, Abhradeep},
  year = {2014},
  month = oct,
  pages = {464--473},
  publisher = {{IEEE}},
  address = {{Philadelphia, PA, USA}},
  doi = {10.1109/FOCS.2014.56},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6979031},
  urldate = {2019-11-06},
  abstract = {Convex empirical risk minimization is a basic tool in machine learning and statistics. We provide new algorithms and matching lower bounds for differentially private convex empirical risk minimization assuming only that each data point's contribution to the loss function is Lipschitz and that the domain of optimization is bounded. We provide a separate set of algorithms and matching lower bounds for the setting in which the loss functions are known to also be strongly convex.},
  file = {/home/pmangold/Documents/references/pdf/bassily_et_al_2014_private_empirical_risk_minimization.pdf},
  isbn = {978-1-4799-6517-5},
  language = {en}
}

@article{chaudhuri2011Differentially,
  title = {Differentially {{Private Empirical Risk Minimization}}},
  author = {Chaudhuri, Kamalika and Monteleoni, Claire and Sarwate, Anand D},
  year = {2011},
  pages = {41},
  abstract = {Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the {$\epsilon$}-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacypreserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.},
  file = {/home/pmangold/Documents/references/pdf/chaudhuri_et_al_2011_differentially_private_empirical_risk_minimization.pdf},
  keywords = {favorite},
  language = {en}
}

@article{dwork2006Differential,
  title = {Differential {{Privacy}}},
  author = {Dwork, Cynthia},
  year = {2006},
  file = {/home/pmangold/Documents/references/pdf/dwork_2006_differential_privacy.pdf},
  journal = {33rd International Colloquium on Automata, Languages and Programming}
}

@article{shokri2017Membership,
  title = {Membership {{Inference Attacks}} against {{Machine Learning Models}}},
  author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  year = {2017},
  month = mar,
  url = {http://arxiv.org/abs/1610.05820},
  urldate = {2020-04-15},
  abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on.},
  archiveprefix = {arXiv},
  eprint = {1610.05820},
  eprinttype = {arxiv},
  file = {/home/pmangold/Documents/references/pdf/shokri_et_al_2017_membership_inference_attacks_against_machine_learning_models.pdf},
  journal = {arXiv:1610.05820 [cs, stat]},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{wang2018Differentially,
  title = {Differentially {{Private Empirical Risk Minimization Revisited}}: {{Faster}} and {{More General}}},
  author = {Wang, Di and Ye, Minwei and Xu, Jinhui},
  year = {2018},
  pages = {10},
  abstract = {In this paper we study the differentially private Empirical Risk Minimization (ERM) problem in different settings. For smooth (strongly) convex loss function with or without (non)-smooth regularization, we give algorithms that achieve either optimal or near optimal utility bounds with less gradient complexity compared with previous work. For ERM with smooth convex loss function in high-dimensional (p n) setting, we give an algorithm which achieves the upper bound with less gradient complexity than previous ones. At last, we generalize the expected excess empirical risk from convex loss functions to non-convex ones satisfying the PolyakLojasiewicz condition and give a tighter upper bound on the utility than the one in [34].},
  file = {/home/pmangold/Documents/references/pdf/wang_et_al_2018_differentially_private_empirical_risk_minimization_revisited.pdf;/home/pmangold/Documents/references/pdf/wang_et_al_2018_differentially_private_empirical_risk_minimization_revisited2.pdf},
  keywords = {favorite},
  language = {en}
}