\documentclass[aspectratio=169,12pt]{beamer}

\usepackage{hyperref}
\usepackage[giveninits=true,doi=false,isbn=false,url=false,eprint=false]{biblatex}


\usepackage{subcaption}

\renewbibmacro*{booktitle}{%
  \textbf{\printfield[booktitle]{booktitle}}%
}


\usepackage{amsmath}
\usepackage{bm}

\usepackage{tcolorbox}

\addbibresource{references.bib}
\addbibresource{publications_oral.bib}

\usepackage{varwidth}
\usepackage{tikz}
\usetikzlibrary{tikzmark}



\tikzset{
    blueblock/.style = {draw=amethyst,fill=amethyst!15,rounded corners=0pt,inner sep=4pt}
}
\tikzset{
    pinkblock/.style = {draw=amaranth,fill=amaranth!15,rounded corners=0pt,inner sep=4pt}
}

\tikzset{
    widepinkblock/.style = {draw=amaranth,fill=amaranth!15,rounded corners=0pt,inner sep=20pt}
}

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}


\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\definecolor{darkspringgreen}{rgb}{0.09, 0.45, 0.27}
\definecolor{amaranth}{rgb}{0.9, 0.17, 0.31}
\definecolor{amethyst}{rgb}{0.6, 0.4, 0.95}

\usepackage{enumitem}

% Labels for items in (nested) itemize (uses bullets/characters)
\newcommand\labelitemi{\textcolor{amaranth}{\textbullet}}% bullet
\newcommand\labelitemii{\textcolor{amaranth}{\normalsize\normalfont\bfseries \textendash}}% --
\newcommand\labelitemiii{\textcolor{amaranth}{\textasteriskcentered}}% *
\newcommand\labelitemiv{\textcolor{amaranth}{\textperiodcentered}}% .
\setlist[enumerate,1]{label={\textcolor{amaranth}{\arabic*.}}}


% footnote size
\makeatletter
\newcommand\notsotiny{\@setfontsize\notsotiny\@vipt\@viipt}
\makeatother

\setbeamerfont{footnote}{size=\notsotiny}


\makeatletter
\newcommand{\Pause}[1][]{\unless\ifmeasuring@\relax
\pause[#1]%
\fi}
\makeatother
%%%%%% Template things

% space between paragraphs
\parskip=1em

% title font
\setbeamerfont{title}{size=\LARGE}%, series=\bfseries}
\setbeamerfont{frametitle}{size=\LARGE}%, series=\bfseries}
\setbeamerfont{institute}{size=\normalsize}%, series=\bfseries}

% spacing between frame title and content
\addtobeamertemplate{frametitle}{\vspace*{0.2cm}}{\vspace*{0.5cm}\setcounter{footnote}{0}}

% color
\definecolor{beamer@blendedblue}{rgb}{0.8, 0, 0.34}%{0.44, 0.16, 0.39}

% no navigation
\beamertemplatenavigationsymbolsempty

% slide numbers
\setbeamertemplate{footline}
{
  \hbox{\begin{beamercolorbox}[wd=1\paperwidth,ht=5.25ex,dp=4ex,right]{framenumber}%
      \large \insertframenumber{}~~
    \end{beamercolorbox}}%
  \vskip0pt%
}

% slide numbers
\def\setupappendix{
\setcounter{framenumber}{0}
\setbeamertemplate{footline}
{
  \hbox{\begin{beamercolorbox}[wd=1\paperwidth,ht=5.25ex,dp=4ex,right]{framenumber}%
      \large A-\insertframenumber{}~~
    \end{beamercolorbox}}%
  \vskip0pt%
}
}


% centered titles
\makeatletter
\long\def\beamer@@frametitle[#1]#2{%
  \beamer@ifempty{#2}{}{%
    \gdef\insertframetitle{\centering{#2\ifnum\beamer@autobreakcount>0\relax{}\space\usebeamertemplate*{frametitle continuation}\fi}}%
  \gdef\beamer@frametitle{#2}%
  \gdef\beamer@shortframetitle{#1}%
}%
}
\makeatother

%%%%% Equations

\newcounter{mytn}
\makeatletter
\newcommand{\tmn}[3][]{\stepcounter{mytn}%
\tikzmarknode[Col\the\numexpr\value{mytn}-\mytn@start\relax/.try,inner xsep=2pt,%
minimum height=1.6em,inner sep=2mm,#1]{mytn-\number\value{mytn}}{#2}%
\expandafter\gdef\csname tmn@annot@\number\value{mytn}\endcsname{#3}}
\newenvironment{AnnotatedEquation}{\edef\mytn@start{\number\value{mytn}}%
\begin{equation*}}{\end{equation*}%
\edef\mytn@end{\number\value{mytn}}%
\ifnum\mytn@end>\mytn@start
\begin{itemize}
 \foreach \X in {\the\numexpr\mytn@start+1,...,\mytn@end}
 {\item \tikzmarknode{mytn-annot-\X}{\csname tmn@annot@\X\endcsname}%
   \begin{tikzpicture}[overlay,remember picture]
  \draw[-stealth] (mytn-annot-\X.east) to[out=0,in=-90] (mytn-\X.south);
 \end{tikzpicture}}
\end{itemize}
\fi}
\makeatother
\tikzset{ Col1/.style= {fill=blue!20,anchor=base,rounded corners=2pt},
Col2/.style= {Col1, fill=red!20},
Col3/.style= {Col1, fill=green!20},
Col4/.style= {Col1, fill=yellow!20},
}

% footnotes at end of frame rather than minipage

\makeatletter
\renewrobustcmd{\blx@mkbibfootnote}[2]{%
  \iftoggle{blx@footnote}
    {\blx@warning{Nested notes}%
     \addspace\mkbibparens{#2}}
    {\unspace
     \ifnum\blx@notetype=\tw@
       \expandafter\@firstoftwo
     \else
       \expandafter\@secondoftwo
     \fi
       {\csuse{blx@theendnote#1}{\protecting{\blxmkbibnote{end}{#2}}}}
       {\csuse{footnote}[frame]{\protecting{\blxmkbibnote{foot}{#2}}}}}}
\makeatother

%%%%% INFORMATION

% \input{src/projet-def}

% \def\firstaxis{Accélerer l’optimisation stochastique fédérée}
% \def\secondaxis{Apprentissage par renforcement fédéré}
% \def\thirdaxis{Enjeux de société en fédéré : confidentialité et équité}

% \def\axisone{\subtleemph{Axe 1}\xspace}
% \def\axistwo{\subtleemph{Axe 2}\xspace}
% \def\axisthree{\subtleemph{Axe 3}\xspace}

% \typeproject{Projet de Rercherche}
% \title{Apprentissage fédéré à l’épreuve du réel\\
% \textcolor{drakgray}{\large Mise en place efficace, nouvelles applications et enjeux de société} }
% \author{Paul Mangold}



\title{\Large Analyse Raffinée de Federated Averaging \\ \Large  et Extrapolation de Richardson-Romberg Fédérée }
\subtitle{}
%Projet de Recherche : 
\vspace{0em}
\author{
  Paul Mangold (CMAP, École Polytechnique)
}
\titlegraphic{
}
\institute{}
\date{Journées de Statistique de la SFdS \\[0.5em] Mercredi 4 juin 2025}


%%%%% DOCUMENT

\begin{document}

%% TITLE PAGE
\begin{frame}[plain]
  \maketitle
\end{frame}

% \begin{frame}[plain]
%   \begin{center}
%     \textcolor{beamer@blendedblue}{
%       \huge Why federated learning?
%     }
%   \end{center}
% \end{frame}






% \begin{frame}{Mes autres travaux}
% \begin{itemize}
%     \item Équité et Confidentialité\footfullcite{mangold2023differential}
%     \item Apprentissage par renforcement fédéré\footfullcite{labbi2025federated}\footfullcite{mancini2024joint}
%     \item Apprentissage fédéré\footfullcite{mangold2020decentralized}\footfullcite{lamer2021specifications}
% \end{itemize}
% \end{frame}

% \appendix
% \setupappendix


% \begin{frame}{Garanties de précision pour DP-SGD}%\footfullcite{bassily2014Private}}
% Si $f$ est convexe, $\Lambda$-Lipschitz, et $\nabla f$ est Lipschitz
%   \begin{align*}
%     \mathbb{E} ( f(x^{\text{(SGD)}}) - f(x^\star) )
%     & =
%       O\Bigg(
%       \frac{
%       \Lambda \|x^\star\|_2 
%       \sqrt{p\log(1/\delta)}}{n \epsilon}
%       \Bigg)
%   \end{align*}

%   % \vspace{1em}

%   % \only<1,2>{\vspace{0.2em}}%
%   % \only<1>{
%   %   \qquad\qquad
%   %   optimization error
%   %   \tikz[remember picture,overlay]{\node[coordinate] (optdef) {};}
%   %   \quad\quad
%   %   privacy error
%   %   \tikz[remember picture,overlay]{\node[coordinate] (privdef) {};}
%   % }
%   % \only<2>{
%   %   ~~~~~~~~after balancing the two terms
%   %   \tikz[remember picture,overlay]{\node[coordinate] (balanceddef) {};}
%   % }
%   % \only<3>{
%     \begin{center}
%       $\Rightarrow$ 
%       sous ces hypothèses, on ne peut pas faire mieux
%     \end{center}
%   % }

% \end{frame}



% \begin{frame}[t]{Problématiques d'équité}

%   \vspace{1.5em}

%   \begin{minipage}{0.47\linewidth}
%     \begin{center}
%       \only<1>{\includegraphics[width=\linewidth]{images/logistic_reg_base.pdf}}%
%       \only<2>{\includegraphics[width=\linewidth]{images/logistic_reg_base_fair.pdf}}%
%       \only<3>{\includegraphics[width=\linewidth]{images/logistic_reg_base_shirt.pdf}}%
%       \only<4>{\includegraphics[width=\linewidth]{images/logistic_reg_base_dress.pdf}}%
%     \end{center}
%   \end{minipage}~~~~~%
%   \begin{minipage}{0.45\linewidth}
%     \textsc{Équité de groupe}:
%     \begin{center}
%       \small \centering
%       Différents groupes de la population peuvent être traités différemment 
%     \end{center}

%   \end{minipage}

%   % \vspace{0.5em}
%   % \begin{overlayarea}{\textwidth}{3cm}
%   % \begin{center}
%   %   Note: Perturbing the model can have a disparate impact\only<5>{\footfullcite{bagdasaryan2019Differential}}
%   % \end{center}
%   % \end{overlayarea}
% \end{frame}




% \begin{frame}{Why share in the first place?}

%   If it is so difficult to share data... why do it?
%   \begin{itemize}
%   \item local datasets are often too small

%     {\small
%       $\rightarrow$ no statistical significance (e.g. medical study)
%     }
%     \\
%     ~
    
    
%   \item local datasets can be biased

%     {\small
%       $\rightarrow$ if a self-driving car learns in countryside, can it drive in the city?
%     }
%     \\
%     ~
    
        
%   \end{itemize}
  
% \end{frame}




% \begin{frame}{Best Scenario: Homogeneous Data}

%   \begin{minipage}[t]{0.5\linewidth}
%     $N$ local sub-problems
%     \small
%     \begin{align*}
%       \min_{\theta \in \mathbb{R}^d}
%       \mathbb{E}_{x^1, y^1 \sim \mathcal{D}^1} \Big[ \ell( \theta; x^1, y^1 ) \Big]
%       \rightarrow
%       \theta_\star^1
%     \end{align*}

%     \vspace{-2em}
    
%     \begin{align*}
%       \min_{\theta \in \mathbb{R}^d}
%       \mathbb{E}_{x^2, y^2 \sim \mathcal{D}^2} \Big[ \ell( \theta; x^2, y^2 ) \Big]
%       \rightarrow
%       \theta_\star^2
%     \end{align*}

%     \vspace{-2em}

%     \begin{align*}
%       \vdots
%     \end{align*}

%     \vspace{-2em}
    
%     \begin{align*}
%       \min_{\theta \in \mathbb{R}^d} 
%       \mathbb{E}_{x^N, y^N \sim \mathcal{D}^N} \Big[ \ell( \theta; x^N, y^N ) \Big]
%       \rightarrow
%       \theta_\star^N
%     \end{align*}
%   \end{minipage}~~~~%
%   \begin{minipage}[t]{0.45\linewidth}
%     \pause
    
%     Estimate global solution
%     \begin{align*}
%       \theta_\star
%       = \frac{1}{N} \sum_{c=1}^N \theta_\star^c
%     \end{align*}

%     OK if $\mathcal{D}_1 = \mathcal{D}_2 = \dots = \mathcal{D}_N$ 
%   \end{minipage}
  
% \end{frame}

% \begin{frame}{Best Scenario: Homogeneous Data}
%   \vspace{-2em}
%   \begin{center}
%     \includegraphics[width=0.8\linewidth]{images_oral/all-minimums-homogeneous.pdf}
%   \end{center}
% \end{frame}

% \begin{frame}{Failure: Heterogeneous Data}
%   \vspace{-2em}
%   \begin{center}
%     \includegraphics[width=0.8\linewidth]{images_oral/all-minimums-heterogeneous.pdf}
%   \end{center}
%   \only<2>{%
%     \tikz[overlay,remember picture]
%     \node[fill=beamer@blendedblue!10,text=black,inner sep=2em,line width=2pt,draw=beamer@blendedblue] at ([xshift=0cm,yshift=0cm]current page.center){\LARGE We need a different method...};
%   }  
% \end{frame}


%\begin{frame}{Communication and Sample Complexity\\[-0.5em]
%  \large I - Homogeneous Data}
  
%\end{frame}


%\begin{frame}{Communication and Sample Complexity\\[-0.5em]
%  \large II - Heterogeneous Data}
  
%\end{frame}



% \begin{frame}{Modern Machine Learning Uses Lots of Data}

% \begin{center}
%     \includegraphics[width=0.6\linewidth]{images/chatgptdata.png}
% \end{center}
    
% \end{frame}


% \begin{frame}{Scaling Laws in Machine Learning}
    
% More data and compute give better models (plots from Kaplan et al., 2020 \footfullcite{kaplan2020scaling})

% \vspace{-1em}

% \begin{center}
%     \includegraphics[width=0.8\linewidth]{images/scalinglaws.png}
% \end{center}

% Question: how to collect enough data and compute... \pause when you are not OpenAI?

% \pause

% \begin{tikzpicture}[remember picture, overlay]
%         \node[widepinkblock]
%         at (current page.center) {
%         \Large
%         Tentative solution: train a model together!
%         };
%     \end{tikzpicture}


% \vspace{1em}

%     % \begin{columns}[T]
%     %     \begin{column}{0.5\textwidth}
%     %         \begin{center}
%     %             % \includegraphics[width=0.8\linewidth]{scaling_laws_graph.png} \\[1em]
%     %             \scriptsize
%     %             \textit{Scaling laws predict that with more data, model accuracy improves.}
%     %         \end{center}
%     %     \end{column}
%     %     \begin{column}{0.5\textwidth}
%     %         \vspace{-2em}
%     %         \small
%     %         \begin{itemize}[leftmargin=*, itemsep=1em]
%     %             \item ChatGPT was trained on hundreds of billions of words, using vast text corpora from books, articles, and websites.
%     %             \item Over 175 billion parameters were learned, requiring massive datasets to reach its performance.
%     %             \item Every day, about 2.5 quintillion bytes of data are generated globally (text, images, videos, etc.).
%     %             \item Federated learning could allow us to tap into this decentralized data, training models on data from individual devices while preserving privacy.
%     %         \end{itemize}
%     %     \end{column}
%     % \end{columns}
    
% \end{frame}


\begin{frame}[t]{Federated Learning}
% \footfullcite{mangold2020decentralized}$^{,}$\footfullcite{ogier2022flamby}$^{,}$\footfullcite{mangold2024scafflsa}$^{,}$\footfullcite{mangold2025refined}$^{,}$\footfullcite{mangold2025scaffold}}
\vspace{0.5em}
  \begin{minipage}{0.35\linewidth}
    \begin{center}
    % \includegraphics[width=\linewidth]{images/federated_learning_only_server.pdf} }%
    % \only<2,3,4>{    
    \includegraphics[width=\linewidth]{images/federated_learning.pdf} 
    % }
    \end{center}
    
  \end{minipage}~~~~~~%
  \begin{minipage}{0.5\linewidth}

%\only<3,4>{
    \begin{center}
      Collaborative optimization problem
      \begin{align*}
        \min_{x \in \mathbb{R}^d} 
        \frac{1}{N} \sum_{c=1}^N f_c(x)
        ~~,
        \quad 
        f_c(x)
        = \mathbb{E}_Z[ F_c(x; Z) ]
      \end{align*}
      % $\rightarrow$ avec une seule \emph{solution globale}
    \end{center}
 %   }
      
  \end{minipage}

% \vspace{0.5em}

% \only<4>{
%   \begin{center}
%       \textbf{\textcolor{purple}{Difficultés centrales}} : hétérogénéité des données et des moyens de calcul 

%       \vspace{-0.5em}
      
%       + communication lente et difficile à établir
%   \end{center}
% }
  \begin{center}
      \textbf{\textcolor{purple}{Central Challenges}}: data and computational heterogeneity 

      \vspace{-0.5em}
      
      + slow and difficult-to-establish communication
  \end{center}
\end{frame}



\begin{frame}
  \begin{center}
    \huge \textcolor{purple}{
      Federated Averaging
      }
  \end{center}
\end{frame}


\begin{frame}[t]{Federated Averaging\footfullcite{mcmahan2017communicationoral} ~~~ \raisebox{0.2em}{\textcolor{black}{\normalsize $x^\star \in \arg\min_{x \in \mathbb{R}^d} 
    \frac{1}{N} \sum_{c=1}^N \mathbb{E}_Z[ F_c(x; Z) ]$}} ~~}  
    
    \vspace{1em}

  \begin{minipage}{0.5\linewidth}

  \footnotesize
  At each global iteration

    \begin{itemize}[leftmargin=*,itemsep=0em]
  \footnotesize
    \item For $c=1$ à $N$ in parallel

\vspace{-0.2em}
    
    
\begin{itemize}[leftmargin=*,itemsep=0em]
\item Receive $x^{(t)}$, set $x^{(t,0)}_c = x^{(t)}$
    
        \item For $h=0$ to $H-1$
    \end{itemize}

\vspace{-0.6em}
\begin{center}
            \hspace{-1em}$x^{(t,h+1)}_c = x^{(t,h)}_c - \gamma \nabla F_c( x^{(t,h)}_c ; Z_c^{(t,h+1)})$
        \end{center}
      
  \item Aggregate local models 
        
        
\vspace{-0.6em}
\begin{center}
            \hspace{-1em}$x^{(t+1)} = \frac{1}{N} \sum_{c=1}^N x_c^{(t,H)}$
        \end{center}
      
    \end{itemize}
      
  \end{minipage}~%
  \begin{minipage}{0.48\linewidth}
%    \pause
    \begin{center}
      \small
    With deterministic gradients:
    \includegraphics[width=0.75\linewidth]{images/local_training_heterogeneous.pdf}%
    \raisebox{2.5em}{ \includegraphics[width=0.25\linewidth]{images/legend.pdf} }
  \end{center}
  
  %   \vspace{-0.5em}

  % ~~~~Plus d'itérations locales

  %   \vspace{0.5em}
    
  %   ~~~~~{\large \textcolor{green} \cmark}\, convergence plus rapide
    
  %   ~~~~~{\large \textcolor{red} \xmark}~ biais plus grand

  \end{minipage}

  \vspace{1.5em}


\end{frame}

\begin{frame}{Classical analyses of this algorithm\\[-0.5em]
    \small (For $L$-smooth, $\mu$-strongly convex functions)}

  Choose your favorite heterogeneity measure
  \pause
  \begin{itemize}
  \item first-order\footfullcite{lian2017can}: $\zeta = \frac{1}{N} \sum_{c=1}^N \big\| \nabla f_c(x^{\star}) - \nabla f(x^{\star}) \big\|^2$
  
  \pause
  
  \item second-order\footfullcite{khaled2022faster}: $\zeta = \frac{1}{N} \sum_{c=1}^N \big\| \nabla_c^2 f(x^{\star}) - \nabla^2 f(x^{\star}) \big\|^2$
  
  \pause
  
  \item average drift\footfullcite{wang2024Unreasonable}: $\zeta = \big\| \frac{1}{NH} \sum_{c=1}^N \sum_{h=0}^{H-1} \nabla f(x_c^{(h)}) - \nabla f(x^{\star}) \big\|^2$
  \end{itemize}

  %\vspace{0.5em}

  \pause

  Show \textcolor{purple}{\bfseries convergence to a neighborhood} of $x^\star$
  \begin{align*}
    \| x^{(T)} - x^\star \|^2 \lesssim (1 - \gamma \mu)^{HT} \| x^{(0)} - x^\star \|^2
    + \textcolor{purple}{\boldsymbol{\chi(\gamma, H, \zeta)}} ~~~~~\text{\small (for some function $\chi$)}
  \end{align*}

  \vspace{1em}

  
\end{frame}


\begin{frame}[t]
  \vspace{-1em} 
\begin{align*}
    \| x^{(T)} - x^\star \|^2 \lesssim (1 - \gamma \mu)^{HT} \| x^{(0)} - x^\star \|^2
    + \textcolor{purple}{\boldsymbol{\chi(\gamma, H, \zeta)}} ~~~~~\text{\small (for some function $\chi$)}
  \end{align*}
  \vspace{-2em} 
  \begin{center}
    
  \resizebox{0.9\linewidth}{!}{
  \begin{tikzpicture}
    \draw [-to,very thick](0,0) -- (12.5,0);
    \node at (1.5,0.2) {$H=2$};
    \node at (6,0.2) {$H=10$};
    \node at (10.5,0.2) {$H=50$};
  \end{tikzpicture}
}
\vspace{-0.5em}

\only<1>{
  \includegraphics[width=0.3\linewidth]{images/plots/fedavg_False_2_t1000_s0.pdf}
  \includegraphics[width=0.3\linewidth]{images/plots/fedavg_False_10_t1000_s0.pdf}
  \includegraphics[width=0.3\linewidth]{images/plots/fedavg_False_50_t1000_s0.pdf}
}%
\only<2>{
  \includegraphics[width=0.3\linewidth]{images/plots/fedavg_True_2_t1000_s0.pdf}
  \includegraphics[width=0.3\linewidth]{images/plots/fedavg_True_10_t1000_s0.pdf}
  \includegraphics[width=0.3\linewidth]{images/plots/fedavg_True_50_t1000_s0.pdf}
}%
% \only<4>{
%   \includegraphics[width=0.3\linewidth]{images/plots/fedavg_True_2_t1000_s1.pdf}
%   \includegraphics[width=0.3\linewidth]{images/plots/fedavg_True_10_t1000_s1.pdf}
%   \includegraphics[width=0.3\linewidth]{images/plots/fedavg_True_50_t1000_s1.pdf}
% }

\vspace{-3em}

\includegraphics[width=0.8\linewidth]{images/legend_fedavg.pdf}    
    
  \end{center}

  \vspace{-3em}

  \begin{center}
    When the number of local iterations increases, bias incrases
    
  \vspace{-0.5em}

    \pause
    
  %   ... but the bound is oblivious to problem's geometry
    

  % \pause

  \vspace{-0.5em}
  
  \textcolor{purple}{\bfseries Remark:} It seems that iterates converge in some way?
  \end{center}
  
\end{frame}

\begin{frame}{Federated Averaging as Fixed Point Iteration}

    Remark that, starting with $x_c^{(t)}, y_c^{(t)} \in \mathbb{R}^d$,
    \begin{align*}
        x_c^{(t,h+1)} - 
        y_c^{(t,h+1)}
        =
        x_c^{(t,h)} -
        y_c^{(t,h)}
        - \gamma ( \nabla f_c( x_c^{(t,h)} ) - \nabla f_c(y_c^{(t,h)}) )
    \end{align*}

    Thus
    \begin{align*}
    \| x_c^{(t+1)} - y_c^{(t+1)} \|
    \le
    (1 - \gamma \mu)^H \| x_c^{(t)} - y_c^{(t)} \|
    \end{align*}

    \pause

\begin{center}
\large 
    \textcolor{purple}{$\Rightarrow$ deterministic FedAvg converges to a unique point\footfullcite{malinovskiy2020local}} 
\end{center}
\end{frame}

\begin{frame}[t]{Open Question: What about the Stochastic Case?}
  \vspace{-1em} 
  \pause

  \begin{center}
    
  \resizebox{0.9\linewidth}{!}{
  \begin{tikzpicture}
    \draw [-to,very thick](0,0) -- (12.5,0);
    \node at (1.5,0.2) {$H=2$};
    \node at (6,0.2) {$H=10$};
    \node at (10.5,0.2) {$H=50$};
  \end{tikzpicture}
}
\vspace{-0.5em}

  \includegraphics[width=0.3\linewidth]{images/plots/fedavg_True_2_t1000_s1.pdf}
  \includegraphics[width=0.3\linewidth]{images/plots/fedavg_True_10_t1000_s1.pdf}
  \includegraphics[width=0.3\linewidth]{images/plots/fedavg_True_50_t1000_s1.pdf}

\vspace{-3em}

\includegraphics[width=0.8\linewidth]{images/legend_fedavg.pdf}    
    
  \end{center}

  \vspace{-3em}

  \pause
  \begin{tikzpicture}[remember picture, overlay]
        \node[widepinkblock]
        at (current page.center) {
        \huge
        Short answer: it also converges!
        };
    \end{tikzpicture}
\end{frame}




\begin{frame}[t]{FedAvg (with stochastic gradients) converges!\footfullcite{mangold2025refined}\\[-0.5em]
      \small (For thrice derivable, $L$-smooth, $\mu$-strongly convex functions)}
    
\begin{itemize}[leftmargin=*]
    \item FedAvg converges to a stationary distribution $\pi^{(\gamma, H)}$
    \only<1>{
    \begin{itemize}
        \item denoting $x^{(t)} \sim \psi_{x^{(t)}}$, we have
    \begin{align*}
        \mathcal{W}_2(\psi_{x^{(t)}}; \pi^{(\gamma, H)})
        \le
        (1 - \gamma \mu)^{H t} \mathcal{W}_2(\psi_{x^{(0)}}; \pi^{(\gamma, H)})
    \end{align*}
    \item where $\mathcal{W}_2$ is the second order Wasserstein distance
    \end{itemize}
    }

\pause

    \item FedAvg's iterates covariance is
    \only<2,3>{
    \begin{align*}
    \int (x - x^\star)(x - x^\star)^\top \pi^{(\gamma, H)}(\mathrm{d} x)
    &  =
    \tikz[remember picture,baseline=(variance.base)] \node[pinkblock] (variance) at (0,0) {
    \textnormal{$\displaystyle \frac{\gamma}{N} C(x^\star) $}}; + O(\gamma^{3/2} H)
    \end{align*}
    }

\only<3>{
\begin{tikzpicture}[overlay,remember picture]
    \node[pinkblock] (variancelegend) at (5,4) {\shortstack{\underline{Linear speed-up !} \\[0.5em]
    variance decreases in $1/N$ \\
    $C(x^\star)$ is $\nabla F^Z$'s covariance at $x^\star$
    }};
    \draw[color=amethyst] (variance) -- (variancelegend);
\end{tikzpicture}

\vspace{-4em}
}
\pause

\pause


    \item We can now give an \textcolor{purple}{\bfseries exact expansion of the bias}
    \only<4,5>{
    \begin{align*}
    \int x \pi^{(\gamma, H)}(\mathrm{d} x)
    &  = x^\star +
    \tikz[remember picture,baseline=(heterbias.base)] \node[blueblock] (heterbias) at (0,0) {
    \textnormal{$\displaystyle \frac{\gamma (H-1)}{2N}
    \sum_{c=1}^N \nabla^2 f(x^\star)^{-1}
    \big( \nabla^2 f_c(x^\star) - \nabla^2 f(x^\star) \big)
    \nabla f_c(x^\star)$}
    };
    \\ 
    &  \qquad
    -  \tikz[remember picture,baseline=(stobias.base)] \node[pinkblock] (stobias) at (0,0) {
    \textnormal{$\displaystyle
    \frac{\gamma}{2 N} \nabla^2 f(x^\star)^{-1}
    \nabla^3 f(x^\star) A^{-1} C(x^\star)
    $}};
    + O(\gamma^{3/2} H)
    \end{align*}
    }
\pause

\only<4,5>{
\begin{tikzpicture}[overlay,remember picture]
    \node[blueblock] (heterbiaslegend) at (3,6) {\shortstack{\underline{Heterogeneity bias} \\[0.5em]
     vanishes when $\nabla^2 f_c(x^\star) = \nabla^2 f(x^\star)$ \\
    \quad or when $\nabla f_c(x^\star) = \nabla f(x^\star)$ \\
    \quad or when $H=1$ (one local update)
    }};
    \draw[color=amethyst] (heterbias) -- (heterbiaslegend);
    
    \node[pinkblock] (stobiaslegend) at (10.5,6) {\shortstack{\underline{Stochasticity bias} \\[0.5em]
    $A = I \otimes \nabla^2 f(x^\star) + \nabla^2 f(x^\star) \otimes I $ \\
    $C(x^\star)$ is $\nabla F^Z$'s covariance at $x^\star$
    }};
    \draw[color=amaranth] (stobias) -- (stobiaslegend);
\end{tikzpicture}

\vspace{-4em}
}

\end{itemize}

\vspace{4em}

\end{frame}


\begin{frame}{Correcting the Bias\\[-0.2em]
\large Novel Algorithm: Federated Richardson-Romberg Extrapolation}
    Run FedAvg twice:
    \begin{itemize}
        \item with step size $\gamma$: global iterates $x_{\gamma}^{(t)}$
        \item with step size $2 \gamma$: global iterates $x_{2 \gamma}^{(t)}$
    \end{itemize}

    We can combine the iterates
    \begin{align*}
        \chi_{\text{RR}}^{(t)}
        =
        2 x_{\gamma}^{(t)}
        - x_{2 \gamma}^{(t)}
    \end{align*}

    \pause
    \begin{tikzpicture}[remember picture, overlay]
        \node[widepinkblock]
        at (current page.center) {
        \shortstack{\large
        Theorem:
        $\mathbb{E} [ \chi_{\text{RR}}^{(t)} ] = x_\star + O(\gamma^2 H^2 + \gamma^{3/2} H)$
        \\[1em]
        \large 
        $\rightarrow$ bias is effectively reduced!!
        }};
    \end{tikzpicture}
\end{frame}

\begin{frame}{Numerical Illustration: FedAvg}
\begin{figure}[t]
    \centering
     \begin{subfigure}[b]{0.45\linewidth}
         \centering
         \includegraphics[width=\textwidth]{images/plots/heterogeneous_iterates_10.pdf}
         \caption{$H=10$}
     \end{subfigure}
     \begin{subfigure}[b]{0.45\linewidth}
         \centering
         \includegraphics[width=\textwidth]{images/plots/heterogeneous_iterates_100.pdf}
         \caption{$H=100$}
     \end{subfigure}
     \end{figure}
     \vspace{-1em}
     \begin{center}
     Blue: FedAvg,~
     Orange: Scaffold,~
     Green: Federated Richardson-Romberg
     \end{center}
\end{frame}


% \begin{frame}
%   \begin{center}
%     \huge \textcolor{purple}{
%       II. Correcting heterogeneity with Scaffold
%       }
%   \end{center}
% \end{frame}


% \begin{frame}[t]{Scaffold\footfullcite{karimireddy2020scaffoldshort} ~~~ \raisebox{0.2em}{\textcolor{black}{\normalsize $x^\star \in \arg\min_{x \in \mathbb{R}^d} 
%     \frac{1}{N} \sum_{c=1}^N \mathbb{E}_Z[ F_c(x; Z) ]$}} ~~}  
    
%     \vspace{-1em}

%   \begin{minipage}{0.5\linewidth}

%   \footnotesize
%   At each global iteration

%     \begin{itemize}[leftmargin=*,itemsep=0em]
%   \footnotesize
%     \item For $c=1$ to $N$ in parallel

% \vspace{-0.2em}
    
    
% \begin{itemize}[leftmargin=*,itemsep=0em]
% \item Receive $x^{(t)}$, set $x^{(t,0)}_c = x^{(t)}$
    
%         \item For $h=0$ to $H-1$
%     \end{itemize}

% \vspace{-0.6em}
% \begin{center}
%             \hspace{-1em}$x^{(t,h+1)}_c = x^{(t,h)}_c - \gamma \big( \nabla F_c( x^{(t,h)}_c ; Z_c^{(t,h+1)}) + \textcolor{purple}{\boldsymbol{\xi_c^{(t)}}} \big)$
%         \end{center}
      
%   \item Aggregate models, update control variates
        
        
% \vspace{-0.6em}
% \begin{center}
%             \hspace{-1em}$x^{(t+1)} = \frac{1}{N} \sum_{c=1}^N x_c^{(t,H)}$
%           \end{center}
% \begin{center}
%             \hspace{-1em}$\textcolor{purple}{\boldsymbol{\xi_c^{(t+1)} = \xi_c^{(t)} + \frac{1}{\gamma H} (\theta_c^{t,H} - \theta^{(t+1)} )}}$
%           \end{center}

          
      
%     \end{itemize}
      
%   \end{minipage}~~~~~~%
%   \begin{minipage}{0.48\linewidth}
%   \pause
%        \begin{center}
%     \includegraphics[width=0.75\linewidth]{images/local_training_homogeneous.pdf}%
%     \raisebox{2.5em}{ \includegraphics[width=0.25\linewidth]{images/legend.pdf} }
%   \end{center}
  
%     \vspace{-0.5em}

%   $\rightarrow$ No more heterogeneity bias!

%   \end{minipage}

%   \vspace{1.5em}

%   \pause
%   \begin{tikzpicture}[remember picture, overlay]
%         \node[widepinkblock]
%         at (current page.center) {
%         \shortstack{
%         \huge
%             Open Question: \\
%         \Large
%             Does linear speed-up remain with control variates?
%             }
%         };
%     \end{tikzpicture}
% \end{frame}




% \begin{frame}[t]{Scaffold also converges !\footfullcite{mangold2025scaffold}\\[-0.5em]
%   \small (For $L$-smooth, $\mu$-strongly convex functions with $\nabla^3 f(x)$ bounded by $Q$)}


% \begin{itemize}[leftmargin=*]
%     \item Scaffold converges if $\gamma H L \le 1$, towards a distribution $\pi^{(\gamma, H)}$
%     \only<1>{
%     \begin{itemize}
%         \item denoting $x^{(t)} \sim \psi_{x^{(t)}}$, we have
%     \begin{align*}
%         \mathcal{W}_2(\psi_{x^{(t)}}; \pi^{(\gamma, H)})
%         \le
%         (1 - \gamma \mu)^{H t} \mathcal{W}_2(\psi_{x^{(0)}}; \pi^{(\gamma, H)})
%     \end{align*}
%     \item where $\mathcal{W}_2$ is the second order Wasserstein distance
%     \end{itemize}
%     }

%     \pause
    
%   \item Scaffold's variance is close to FedAvg's variance
%     \only<2,3>{
%       \begin{align*}
%         \int (x - x^\star)(x - x^\star)^\top \pi^{(\gamma, H)}(\mathrm{d} x)
%         &  =
%           \tikz[remember picture,baseline=(variance.base)] \node[pinkblock] (variance) at (0,0) {
%           \textnormal{$\displaystyle \frac{\gamma}{N} C(x^\star) $}}; + O(\gamma^{3/2})
%       \end{align*}
%     }
%     \pause
    
%     \only<3>{
%       \begin{tikzpicture}[overlay,remember picture]
%         \node[pinkblock] (variancelegend) at (5,4) {\shortstack{\underline{Linear speed-up !} \\[0.5em]
%             variance decreases in $1/N$ \\
%             variance scales in $\gamma$
%           }};
%         \draw[color=amethyst] (variance) -- (variancelegend);
%       \end{tikzpicture}
      
%       \vspace{-4em}
%     }

%     \pause

%   \item Scaffold still has some bias
%     \only<4,5>{
%       \begin{align*}
%         \int x \pi^{(\gamma, H)}(\mathrm{d} x)
%         &  = x^\star
%           -  \tikz[remember picture,baseline=(stobias.base)] \node[pinkblock] (stobias) at (0,0) {
%           \textnormal{$\displaystyle
%           \frac{\gamma}{2 N} \nabla^2 f(x^\star)^{-1}
%           \nabla^3 f(x^\star) A^{-1} C(x^\star)
%           $}};
%           + O(\gamma^{3/2})
%       \end{align*}
%     }

%     \only<5>{
%       \begin{tikzpicture}[overlay,remember picture]    
%         \node[pinkblock] (stobiaslegend) at (10.5,4) {\shortstack{\underline{Stochasticity bias remains} \\[0.5em]
%             $A = I \otimes \nabla^2 f(x^\star) + \nabla^2 f(x^\star) \otimes I $ \\
%     $C(x^\star)$ is $\nabla F^Z$'s covariance at $x^\star$
%           }};
%         \draw[color=amaranth] (stobias) -- (stobiaslegend);
%       \end{tikzpicture}

%       \vspace{-4em}
%     }

% \pause

% \end{itemize}

% \end{frame}
% \begin{frame}[t]

%   \vspace{1em}
  
%   \begin{center}
    
%   \resizebox{0.9\linewidth}{!}{
%   \begin{tikzpicture}
%     \draw [-to,very thick](0,0) -- (12.5,0);
%     \node at (1.5,0.2) {$H=10$};
%     \node at (6,0.2) {$H=20$};
%     \node at (10.5,0.2) {$H=50$};
%   \end{tikzpicture}
% }
% \vspace{-0.5em}

% \only<1>{
%   \includegraphics[width=0.3\linewidth]{images/plots/fedavg_scaffold_True_2_t1000_s0.pdf}
%   \includegraphics[width=0.3\linewidth]{images/plots/fedavg_scaffold_True_10_t1000_s0.pdf}
%   \includegraphics[width=0.3\linewidth]{images/plots/fedavg_scaffold_True_50_t1000_s0.pdf}
% }%
% \only<2>{
%   \includegraphics[width=0.3\linewidth]{images/plots/fedavg_scaffold_True_2_t1000_s1.pdf}
%   \includegraphics[width=0.3\linewidth]{images/plots/fedavg_scaffold_True_10_t1000_s1.pdf}
%   \includegraphics[width=0.3\linewidth]{images/plots/fedavg_scaffold_True_50_t1000_s1.pdf}
% }


% \vspace{-3em}

% \includegraphics[width=\linewidth]{images/legend_fedavg_scaffold.pdf}    

% \end{center}

%   \vspace{-3em}
%   \begin{center}
%     Scaffold converges to the right point


%     ... and its variance is similar to FedAvg!
%   \end{center}


% \end{frame}


% \begin{frame}{New Convergence Rate for Scaffold\\[-0.5em]
%     \small (For $L$-smooth, $\mu$-strongly convex functions with $\nabla^3 f(x)$ bounded by $Q$)}
% \begin{align*}
% \mathbb{E}\left[ \| x^{(T)} - x^\star \|^2 \right] 
% & \lesssim{} \left( 1 - \frac{\gamma \mu}{4} \right)^{HT}
% \left\{ 
% \| x^{(0)} - x^\star \|^2
% +
% 2\gamma^2 H^2 \zeta^2
% +
% \frac{ \sigma_\star^2 }{L\mu}
% \right\}
% \\ &  \quad\quad\quad\quad\quad\quad\quad
% + 
% \frac{\gamma }{\textcolor{purple}{\boldsymbol{N}} \mu} \sigma_\star^2 
% + \frac{ \gamma^{3/2} Q }{\mu^{5/2}} \sigma_\star^3
% + \frac{ \gamma^3 H Q^2}{\mu^3} \sigma_\star^4
% \end{align*}


% where
% \begin{itemize}
% \item \small
%   $\sigma_\star^2 = \mathbb{E}[ \frac{1}{N} \sum_{c=1}^N \| \nabla F_c^Z(x^\star) - \nabla f_c(x^\star) \|^2$ is the variance at $x^\star$
% \item \small
%   $\zeta^2 = \frac{1}{N} \sum_{c=1}^N \| \nabla f_c^Z(x^\star) \|^2$ measures gradient heterogeneity
% \end{itemize}



% \end{frame}

% % {
% % \setbeamercolor{background canvas}{bg=violet!10}
% % \begin{frame}{Proof of the convergence rate (I)}
    
% % \end{frame}

% % \begin{frame}{Proof of the convergence rate (II)}
    
% % \end{frame}
% % }

% \begin{frame}{Linear Speed-Up!}

%   As long as $N$ is not too large, one can obtain $\mathbb{E}\left[ \| x^{(T)} - x^\star \|^2 \right] \le \epsilon^2 $ with
%   \begin{align*}
%     \# \text{grad per client}
%     = \widetilde O\Big( \frac{\sigma_\star^2}{\textcolor{purple}{\boldsymbol{N}} \mu^2 \epsilon^2} 
%     \log\left( 
%     \frac{1}{\epsilon}
%     \right) \Big)
%   \end{align*}
  
% \end{frame}


% \begin{frame}{Numerical Illustration: Speed-Up of Scaffold}
%     \begin{figure}
%         \centering
%         \raisebox{1.5\height}{\rotatebox[origin=c]{90}{\scriptsize MSE}}
%         \begin{subfigure}[t]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{images/plots/linear5000_0.05_10_10.pdf}
        
%         \scriptsize Communication rounds
%         \caption{$N=10$}
%     \end{subfigure}~%
%     \begin{subfigure}[t]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{images/plots/linear5000_0.05_10_100.pdf}
%         \scriptsize Communication rounds
%         \caption{$N=100$}
%     \end{subfigure}~%
%     \begin{subfigure}[t]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{images/plots/linear5000_0.05_10_1000.pdf}
%         \scriptsize Communication rounds
%         \caption{$N=1000$}
%     \end{subfigure}~%
%     \begin{subfigure}[t]{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/plots/linear5000_0.05_10_10000.pdf}
%         \scriptsize Communication rounds
%         \caption{$N=10000$}
%     \end{subfigure}
%     \end{figure}
%      \vspace{-1em}
%      \begin{center}
%      Blue: FedAvg,~
%      Orange: Scaffold
%      \end{center}
% \end{frame}

\begin{frame}{Conclusion}

  \begin{itemize}[itemsep=1em]
  \item FedAvg converges (even with stochastic gradients)
  \item This allows to derive new analyses for these problems,

    with exact first-order expression for bias
  % \item And we proved that Scaffold has:
  %   \begin{itemize}
  %   \item variance similar to FedAvg's variance
  %   \item \textit{linear speed-up} in the number of clients!!
  %   \end{itemize}
    
  %   But... Scaffold is still biased: some good directions for future. :)
  \item FedAvg (and friends) are still biased!
    \begin{itemize}
    \item there is still a lot to do in federated optimization
    \item ... especially when gradients are stochastic, \\
      \qquad ... which is the most interesting setting!
    \end{itemize}

  \item Similar results hold for Scaffold (see next slide)
  \end{itemize}
  
\end{frame}


\begin{frame}{PS: Extension to Scaffold}
  Little teaser :)
  
  Similar results hold for Scaffold, and we prove that:
  \begin{itemize}
  \item Scaffold's iterates converge
  \item Scaffold eliminates heterogeneity bias \emph{but not stochasticity bias}
    \item new convergence rate for Scaffold:
      \begin{align*}
        \mathbb{E}\left[ \| x^{(T)} - x^\star \|^2 \right] 
        & \lesssim{} \left( 1 - \frac{\gamma \mu}{4} \right)^{HT}
          \| x^{(0)} - x^\star \|^2
             + O\Big( \frac{\gamma }{\textcolor{purple}{\boldsymbol{N}}}
             + \gamma^{3/2}
             \Big)
      \end{align*}
      
    \end{itemize}
\end{frame}


\begin{frame}
  \begin{center}
    \huge \textcolor{purple}{
      Thank you!
      }
    \end{center}

    Papers related to this presentation:
    \vspace{-1em}
    \begin{itemize}
    \item \small \fullcite{mangold2025refined}
    \item \small \fullcite{mangold2025scaffold}
    \end{itemize}

    % Find this presentation on my website:
    % \vspace{-1em}
    % \begin{itemize}
    % \item \textcolor{amaranth}{\url{https://pmangold.fr/research.php?page=talks}}
    % \end{itemize}
    
\end{frame}

% \begin{frame}{Federated Learning in the Real World}

%     \vspace{-0.5em}

%     \begin{itemize}[leftmargin=*,itemsep=1em]
%     \small
%         \item Research project, three axes:
%         \begin{enumerate}[label=\textcolor{amaranth}{\Roman*}.]
%     \small
%         \item Accelerating federated stochastic optimization
%         \item Federated reinforcement learning
%         \item Societal challenges in federated learning: privacy and fairness
%     \end{enumerate}
%     \end{itemize}

%     \vspace{0.5em}

%     \begin{itemize}[leftmargin=*,itemsep=1em]
%     \addtocounter{enumi}{1}
%     \small
%         \item PhD 2020--2023 (CRIStAL, Lille), Post-doc 2023--present (CMAP, Paris)
%         % \item Integration project: LAAS (UPR 8001), DI ENS (UMR 8548)
%         \item Specializations: stochastic optimization, federated optimization, reinforcement learning, private optimization, fairness in machine learning
%         \item Open science: my papers, presentation materials, and codes I contributed to are available online at \url{https://pmangold.fr/research.php}
%     \end{itemize}
% \end{frame}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
