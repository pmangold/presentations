Title: Exploiting Problem Structure in Privacy-Preserving Optimization and Machine Learning

Abstract:
In recent decades, concerns about the societal impact of machine learning have been growing. These concerns notably encompass data privacy and fairness of predictions. Guaranteeing data privacy typically reduces model utility: it is thus challenging to learn useful models while preserving privacy. This thesis explores new methods that improve utility for a given privacy guarantee by leveraging structural properties of problems, and studies the impact of privacy on fairness.

The first two contributions of this thesis are two new differentially private optimization algorithms, that are both based on coordinate descent. They aim at exploiting different structural properties of the problem at hand. The first algorithm is based on stochastic coordinate descent, and can exploit imbalance in the scale of the gradient’s coordinates by using large step sizes. The second algorithm is based on greedy coordinate descent, which allows to focus on the most important coordinates of the problem, which can sometimes drastically improve utility (e.g., when the solution of the problem is sparse).

The third contribution of this thesis studies the interplay of differential privacy and fairness. These two notions have rarely been studied simultaneously, and there are growing concerns that differential privacy may exacerbate unfairness. We show that group fairness measures have interesting regularity properties, which allows to derive an upper bound on the difference in fairness levels between a private model and its non-private counterpart.

Titre:
Exploitation de la Structure des Problèmes en Optimisation et en Apprentissage Automatique Respectueux de la Vie Privée

Résumé:
Ces dernières décennies, les inquiétudes concernant l'impact sociétal de l'apprentissage machine n'ont cessé d'augmenter. Ces inquiétudes concernent tout particulièrement la confidentialité des données (privacy) et l'équité des prédictions des modèles appris (fairness). S'il est possible d'entrainer des modèles tout en garantissant une forme de confidentialité dess données, cela réduit généralement la précision des prédictions du modèle. Cette thèse explore de nouvelles méthodes d'apprentissage machine visant à exploiter les propriétés structurelles des problèmes, afin de maximiser la précision des modèles pour une garantie de confidentialité fixée. L'impact de ce type de méthodes sur l'équité des prédictions des modèles ainsi appris sera ensuite étudiée.

Les deux premières contributions de cette thèse sont deux nouveaux algorithmes d'optimisation différencielle privée, tous deux basés sur la descente de coordonnées. Ils visent à exploiter différentes propriétés structurelles du problème en question. Le premier algorithme repose sur la descente de coordonnées stochastique et peut exploiter les déséquilibres dans l'échelle des coordonnées du gradient en utilisant des pas de grande taille. Le second algorithme repose sur la descente de coordonnées vorace, ce qui permet de se concentrer sur les coordonnées les plus importantes du problème, ce qui peut parfois considérablement améliorer l'utilité (par exemple, lorsque la solution du problème est parcimonieuse).

La troisième contribution de cette thèse se penche sur l'interaction entre la confidentialité différentielle et l'équité. Ces deux notions ont rarement été étudiées simultanément, et il y a des inquiétudes croissantes selon lesquelles la confidentialité différentielle pourrait aggraver l'injustice. Nous montrons que les mesures d'équité de groupe présentent des propriétés de régularité intéressantes, ce qui permet de dériver une limite supérieure sur la différence de niveaux d'équité entre un modèle privé et son homologue non privé.
